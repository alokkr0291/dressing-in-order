{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxcLBG9aa54D"
   },
   "source": [
    "The __Official__ Colab Demo for ICCV'21 paper\n",
    "\n",
    "__[Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-on and Outfit Editing](https://arxiv.org/abs/2104.07021)__\n",
    "\n",
    "\n",
    "\\[[Code](https://github.com/cuiaiyu/dressing-in-order)\\]\n",
    "\\[[Paper](https://arxiv.org/abs/2104.07021)\\]\n",
    "\n",
    "# Read Before Starting\n",
    "- This Colab Demo is available for __non-commercial research purposes__ only.\n",
    "- This Colab contains data downloading scripts. __*Please make sure you are legally allowed to use the [DeepFahsion-MultiModal dataset](https://github.com/yumingj/DeepFashion-MultiModal) as required by [their license]() before trying this demo.*__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFP7Kc8HGnb4"
   },
   "source": [
    "# Step 0: Install Environment\n",
    "(This demo only supports inference, so we don't install GFLA.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrc6NTjyUDhs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1733208234204,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "FxLGbnKdGwxP",
    "outputId": "76b7cf01-b402-479e-f4f9-209d4d56b916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  3 06:43:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU\n",
    "# If you don't have GPU, please set it by Runtime -> Change runtime type\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1733208238509,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "YEaYleTmGNiT",
    "outputId": "979821f8-efde-4e73-9e99-d9077e4da13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'dressing-in-order' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# install DiOr\n",
    "import os\n",
    "!git clone https://github.com/cuiaiyu/dressing-in-order\n",
    "repo_name='dressing-in-order'\n",
    "os.chdir(f'./{repo_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6348,
     "status": "ok",
     "timestamp": 1733208245447,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "UoatprfDIh08",
    "outputId": "6ac5412d-eacc-49f5-ad91-27a1374987db"
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-image==0.16.2\n",
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9q_yUPNKTBj"
   },
   "source": [
    "# Step 1: Download Data\n",
    "This Data is from [DeepFashion-Multimodal](https://arxiv.org/abs/2205.15996) [1].\n",
    "\n",
    "Please make sure you read [the dataset license](https://github.com/yumingj/DeepFashion-MultiModal/blob/main/LICENSE) before running the below code.\n",
    "\n",
    "You are __responsible__ to make sure whether the data is available for your usage.\n",
    "\n",
    "```[1] Jiang, Yuming, et al. \"Text2human: Text-driven controllable human image generation.\" ACM Transactions on Graphics (TOG) 41.4 (2022): 1-11.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd dressing-in-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of dress_in_order_demo(1).ipynb ffff.txt\n",
      "LICENSE                              generate_all.py\n",
      "Pose123.png                          \u001b[34mmodels\u001b[m\u001b[m\n",
      "README.md                            \u001b[34moptions\u001b[m\u001b[m\n",
      "\u001b[34mcheckpoints\u001b[m\u001b[m                          output_image.png\n",
      "\u001b[34mcover_images\u001b[m\u001b[m                         requirements.txt\n",
      "\u001b[34mdata\u001b[m\u001b[m                                 \u001b[34mscripts\u001b[m\u001b[m\n",
      "\u001b[34mdatasets\u001b[m\u001b[m                             \u001b[34mtools\u001b[m\u001b[m\n",
      "demo.ipynb                           train.py\n",
      "\u001b[34mdressing-in-order\u001b[m\u001b[m                    \u001b[34mutils\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "09d7e1caadca461585867047d5990d36",
      "4aaa6fd816354d00aaf5dfbb787521ff",
      "148c216158054429965ac0fa9a522738",
      "172df104dabc47ce97e4526eb8c56dd1",
      "581eaef9466e4e0187a7a4dd86539e48",
      "781a90e9055f4e849694f54925153624",
      "510a903806dc49c5a1a1e5417b6f531e",
      "645074065cba4dcca84ea4c51467d30b",
      "c258e81d26334c71a7f2331ab9b04e94",
      "687900a5fd6f4ba79c3a83a05c17fda2",
      "41663c7c73ba4764a37bb0c6e9e27552"
     ]
    },
    "executionInfo": {
     "elapsed": 6291,
     "status": "ok",
     "timestamp": 1733208251727,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "q8hcmJInL0ho",
    "outputId": "a296fa2a-5a5d-4911-851e-4f8099b022e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download testM_lip.\n",
      "download images.\n",
      "download fasion-pairs-test.csv.\n",
      "download fasion-annotation-test.csv.\n",
      "download standard_test_anns.txt.\n"
     ]
    }
   ],
   "source": [
    "# download data from https://github.com/yumingj/DeepFashion-MultiModal\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if not os.path.exists(\"data/\"):\n",
    "  os.system(\"mkdir data\")\n",
    "\n",
    "def download_from_gdrive(dst_root, fn, gdrive_path, iszip=True):\n",
    "  if not os.path.exists(dst_root):\n",
    "    os.system(\"mkdir {}\".format(dst_root))\n",
    "  if not os.path.exists(\"{}/{}\".format(dst_root, fn)):\n",
    "    os.system(\"gdown {}\".format(gdrive_path))\n",
    "    if iszip:\n",
    "      os.system(\"unzip {}.zip\".format(fn))\n",
    "      os.system(\"rm {}.zip\".format(fn))\n",
    "    os.system(\"mv {} {}/\".format(fn, dst_root))\n",
    "  print(\"download {}.\".format(fn))\n",
    "\n",
    "# download data\n",
    "download_from_gdrive(\"data\", \"testM_lip\", \"1toeQwAe57LNPTy9EWGG0u1XfTI7qv6b1\")\n",
    "download_from_gdrive(\"data\", \"images\", \"1U2PljA7NE57jcSSzPs21ZurdIPXdYZtN\")\n",
    "download_from_gdrive(\"data\",\"fasion-pairs-test.csv\",\"12fZKGf0kIu5OX3mjC-C3tptxrD8sxm7x\",iszip=False)\n",
    "download_from_gdrive(\"data\",\"fasion-annotation-test.csv\",\"1MxkVFFtNsWFshQp_TA7qwIGEUEUIpYdS\",iszip=False)\n",
    "download_from_gdrive(\"data\",\"standard_test_anns.txt\",\"19nJSHrQuoJZ-6cSl3WEYlhQv6ZsAYG-X\",iszip=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of images  40659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb82c108a621462580f03a7dd4d83f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter images (exclude training data and rename the files)\n",
    "if not os.path.exists(\"data/test\"):\n",
    "  os.mkdir(\"data/test\")\n",
    "target_fns = [fn[:-4] for fn in os.listdir(\"data/testM_lip\")]\n",
    "print(\"Len of images \" , len(os.listdir(\"data/images/\")))\n",
    "for fn in tqdm(os.listdir(\"data/images\")):\n",
    " # print(fn)\n",
    "  elements = fn.split(\"-\")\n",
    "  if len(elements) >= 3:  \n",
    "      elements[2] = elements[2].replace(\"_\",\"\")\n",
    "      last_elements = elements[-1].split(\"_\")\n",
    "      elements[-1] = last_elements[0] + \"_\" + last_elements[1] + last_elements[2]\n",
    "      new_fn = \"fashion\"+\"\".join(elements)\n",
    "\n",
    "  if new_fn[:-4] in target_fns:\n",
    "    os.system(\"mv {} {}\".format(\"data/images/\"+fn, \"data/test/\"+new_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "executionInfo": {
     "elapsed": 710367,
     "status": "ok",
     "timestamp": 1733208209125,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "QgKewEzONEuG",
    "outputId": "10e946ea-bb66-438f-e4a4-0a800bb2ff44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-image --upgrade\n",
    "#!pip install scikit-image==0.18.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-CZ6KMscZzN"
   },
   "source": [
    "# Step 2: Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8485,
     "status": "ok",
     "timestamp": 1733208263688,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "fOFBTL6sX1tP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/dressinorder/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/dressinorder/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.dior_model import DIORModel\n",
    "import os, json\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5732,
     "status": "ok",
     "timestamp": 1733208272890,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "XnAD8DAHHKcD",
    "outputId": "07bd52c0-381a-49aa-8e76-402abf8da86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download DIORv1_64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vgg ckpt from torchvision dict.\n",
      "[init] init pre-trained model vgg.\n",
      "Initializing network on device: mps\n",
      "initialize network with orthogonal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torch/nn/init.py:610: UserWarning: The operator 'aten::linalg_qr.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1733146601134/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  q, r = torch.linalg.qr(flattened)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network on device: mps\n",
      "initialize network with orthogonal\n",
      "Initializing network on device: mps\n",
      "initialize network with kaiming\n",
      "Initializing network on device: mps\n",
      "initialize network with orthogonal\n",
      "Initializing network on device: mps\n",
      "[init] frozen net netVGG.\n",
      "[init] frozen net netFlow.\n",
      "[init] frozen net netE_attr.\n",
      "[init] frozen net netE_attr.\n",
      "loading the model from checkpoints/DIORv1_64/latest_net_E_attr.pth\n",
      "loading the model from checkpoints/DIORv1_64/latest_net_G.pth\n",
      "not exsits checkpoints/DIORv1_64/latest_net_VGG.pth\n",
      "loading the model from checkpoints/DIORv1_64/latest_net_Flow.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network E_attr] Total number of parameters : 1.191 M\n",
      "[Network G] Total number of parameters : 16.501 M\n",
      "[Network VGG] Total number of parameters : 0.113 M\n",
      "[Network Flow] Total number of parameters : 6.608 M\n",
      "-----------------------------------------------\n",
      "[tensorboard] init tensorboard @ checkpoints/DIORv1_64/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alokkumar/MyDrive/PythonResearch /Github-Downloads /DeepFashionMultimodal/DressInOrderSecondTry/dressing-in-order/models/base_model.py:316: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=torch.device('mps'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = 'data'\n",
    "exp_name = 'DIORv1_64' # DIOR_64\n",
    "epoch = 'latest'\n",
    "netG = 'diorv1' # dior\n",
    "ngf = 64\n",
    "\n",
    "## this is a dummy \"argparse\"\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "if True:\n",
    "    opt = Opt()\n",
    "    opt.dataroot = dataroot\n",
    "    opt.isTrain = False\n",
    "    opt.phase = 'test'\n",
    "    opt.n_human_parts = 8; opt.n_kpts = 18; opt.style_nc = 64\n",
    "    opt.n_style_blocks = 4; opt.netG = netG; opt.netE = 'adgan'\n",
    "    opt.ngf = ngf\n",
    "    opt.norm_type = 'instance'; opt.relu_type = 'leakyrelu'\n",
    "    opt.init_type = 'orthogonal'; opt.init_gain = 0.02; opt.gpu_ids = [0]\n",
    "    opt.frozen_flownet = True; opt.random_rate = 1; opt.perturb = False; opt.warmup=False\n",
    "    opt.name = exp_name\n",
    "    opt.vgg_path = ''; opt.flownet_path = ''\n",
    "    opt.checkpoints_dir = 'checkpoints'\n",
    "    opt.frozen_enc = True\n",
    "    opt.load_iter = 0\n",
    "    opt.epoch = epoch\n",
    "    opt.verbose = False\n",
    "\n",
    "# create model\n",
    "#os.mkdir(\"checkpoints\")\n",
    "download_from_gdrive(\"checkpoints\", \"DIORv1_64\", \"1MyHq-P0c8zz7ey7p_HTTZKeMie5ZuNlb\")\n",
    "\n",
    "model = DIORModel(opt)\n",
    "model.setup(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1733209059763,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "eW-7_qnVYOky"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from datasets.deepfashion_datasets import DFVisualDataset\n",
    "Dataset = DFVisualDataset\n",
    "ds = Dataset(dataroot=dataroot, dim=(256,176), n_human_part=8)\n",
    "\n",
    "# preload a set of pre-selected models defined in \"standard_test_anns.txt\" for quick visualizations\n",
    "inputs = dict()\n",
    "for attr in ds.attr_keys:\n",
    "    inputs[attr] = ds.get_attr_visual_input(attr)\n",
    "\n",
    "# define some tool functions for I/O\n",
    "def load_img(pid, ds):\n",
    "    if len(pid[0]) < 10: # load pre-selected models\n",
    "        person = inputs[pid[0]]\n",
    "        person = (i.to(\"mps\") for i in person)\n",
    "        pimg, parse, to_pose = person\n",
    "        pimg, parse, to_pose = pimg[pid[1]], parse[pid[1]], to_pose[pid[1]]\n",
    "    else: # load model from scratch\n",
    "        person = ds.get_inputs_by_key(pid[0])\n",
    "        person = (i.to(\"mps\") for i in person)\n",
    "        pimg, parse, to_pose = person\n",
    "    return pimg.squeeze(), parse.squeeze(), to_pose.squeeze()\n",
    "\n",
    "\n",
    "# define dressing-in-order function (the pipeline)\n",
    "def dress_in_order(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2], perturb=False):\n",
    "    PID = [0,4,6,7]\n",
    "    GID = [2,5,1,3]\n",
    "    # encode person\n",
    "    pimg, parse, from_pose = load_img(pid, ds)\n",
    "    if perturb:\n",
    "        pimg = perturb_images(pimg[None])[0]\n",
    "    if not pose_id:\n",
    "        to_pose = from_pose\n",
    "    else:\n",
    "        to_img, _, to_pose = load_img(pose_id, ds)\n",
    "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
    "\n",
    "    # encode base garments\n",
    "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
    "\n",
    "\n",
    "    # swap base garment if any\n",
    "    gimgs = []\n",
    "    for gid in gids:\n",
    "        _,_,k = gid\n",
    "        gimg, gparse, pose =  load_img(gid, ds)\n",
    "        seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        gsegs[gid[2]] = seg\n",
    "        gimgs += [gimg * (gparse == gid[2])]\n",
    "\n",
    "    # encode garment (overlay)\n",
    "    garments = []\n",
    "    over_gsegs = []\n",
    "    oimgs = []\n",
    "    for gid in ogids:\n",
    "        oimg, oparse, pose = load_img(gid, ds)\n",
    "        oimgs += [oimg * (oparse == gid[2])]\n",
    "        seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        over_gsegs += [seg]\n",
    "\n",
    "    gsegs = [gsegs[i] for i in order] + over_gsegs\n",
    "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "    return pimg, gimgs, oimgs, gen_img[0], to_pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_img(pimg=[], gimgs=[], oimgs=[], gen_img=[], pose=None, plotName = \"output_img\"):\n",
    "    # Process pose if provided\n",
    "    if pose is not None:\n",
    "        import utils.pose_utils as pose_utils\n",
    "        print(\"Pose shape:\", pose.size())\n",
    "        kpt = pose_utils.draw_pose_from_map(pose.cpu().numpy().transpose(1, 2, 0), radius=6)[0]\n",
    "        kpt = np.clip(kpt / 255.0, 0, 1)  # Normalize to [0, 1]\n",
    "\n",
    "    # Convert inputs to lists if they are not already\n",
    "    if not isinstance(pimg, list):\n",
    "        pimg = [pimg]\n",
    "    if not isinstance(gen_img, list):\n",
    "        gen_img = [gen_img]\n",
    "    \n",
    "    # Combine all images for display\n",
    "    out_images = pimg + gimgs + oimgs + gen_img\n",
    "    if out_images:\n",
    "        # Concatenate along width\n",
    "        out = torch.cat(out_images, dim=2).float().cpu().detach().numpy()\n",
    "        out = (out + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "        out = np.transpose(out, [1, 2, 0])  # Convert to HWC format\n",
    "\n",
    "        # Clip to valid range for visualization\n",
    "        out = np.clip(out, 0, 1)\n",
    "\n",
    "        # Append pose visualization if available\n",
    "        if pose is not None:\n",
    "            out = np.concatenate((kpt, out), axis=1)\n",
    "    else:\n",
    "        # If no image is provided, just display the pose\n",
    "        out = kpt\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=100, facecolor='w', edgecolor='k')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(out)\n",
    "    plt.savefig(plotName+'.png')  # Save as a PNG file\n",
    "    #plt.close(fig)  # Close the figure after saving\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKY9RFe9icn6"
   },
   "source": [
    "# Applications\n",
    "\n",
    "NOTE: INDEX OF GARMENT is used as following:\n",
    "\n",
    "- 'top':5, # dress is also considered as top.\n",
    "- 'bottom':1,\n",
    "- 'hair':2,\n",
    "- 'jacket':3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_GE-cGraZxV"
   },
   "source": [
    "## Pose Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1853,
     "status": "ok",
     "timestamp": 1733210730802,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "Taih27BjKSNM",
    "outputId": "cee80fd3-4e83-47aa-bd57-b4f83d575956"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dressinorder/lib/python3.10/site-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose shape: torch.Size([18, 256, 176])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alokkumar/MyDrive/PythonResearch /Github-Downloads /DeepFashionMultimodal/DressInOrderSecondTry/dressing-in-order/utils/pose_utils.py:93: FutureWarning: `draw.circle` is deprecated in favor of `draw.disk`.`draw.circle` will be removed in version 0.19\n",
      "  yy, xx = circle(joint[0], joint[1], radius=radius, shape=img_size)\n",
      "2024-12-06 12:30:38.353 python[1619:302919] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-06 12:30:38.353 python[1619:302919] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# person id\n",
    "pid = (\"print\", 0, None) # load the 0-th person from \"print\" group, NONE (no) garment is interested\n",
    "# pose id (take this person's pose)\n",
    "pose_id = (\"print\", 2, None) # load the 2-nd person from \"print\" group, NONE (no) garment is interested\n",
    "# generate\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
    "plot_img(pimg, gimgs, oimgs, gen_img, pose, \"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krq8Rf4vaeNE"
   },
   "source": [
    "## Virtual Try-On (Tucking in/out)\n",
    "\n",
    "Users can control the tuck-in and tuck-out results when there is overlap between the top garment and the bottom garment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1733208772441,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "bOyBfIsyHTTx",
    "outputId": "afb40e47-a86f-4366-df16-946c39fa5ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose shape: torch.Size([18, 256, 176])\n",
      "Pose shape: torch.Size([18, 256, 176])\n"
     ]
    }
   ],
   "source": [
    "pid = (\"pattern\", 3, None) # load the 3-rd person from \"pattern\" group, NONE (no) garment is interested\n",
    "gids = [\n",
    "   (\"plaid\",0,5), # load the 0-th person from \"plaid\" group, garment #5 (top) is interested\n",
    "   (\"pattern\",3,1),  # load the 3-rd person from \"pattern\" group, garment #1 (bottom) is interested\n",
    "       ]\n",
    "\n",
    "# tuck in (dressing order: hair, top, bottom)\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, gids=gids, order=[2,5,1])\n",
    "plot_img(pimg, gimgs, gen_img=gen_img, pose=pose)\n",
    "\n",
    "# not tuckin (dressing order: hair, bottom, top)\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, gids=gids, order=[2,1,5])\n",
    "plot_img(pimg, gimgs, gen_img=gen_img, pose=pose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4QfUWboazWN"
   },
   "source": [
    "## Virtual Try-On (Layering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1733208334089,
     "user": {
      "displayName": "alok K",
      "userId": "02982539966379047783"
     },
     "user_tz": -330
    },
    "id": "HLbUttklH3OI",
    "outputId": "19d8f7e7-8d67-438d-a65a-751f264a232a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose shape: torch.Size([18, 256, 176])\n"
     ]
    }
   ],
   "source": [
    "pid = ('plaid',3, 5)\n",
    "ogids = [('print', 2, 5)]\n",
    "# tuck in\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, ogids=ogids)\n",
    "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NetiYTbTgRJw"
   },
   "source": [
    "## Virual Try-On (Layering - Muliple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1681790158125,
     "user": {
      "displayName": "Aiyu Cui",
      "userId": "13130403645654724387"
     },
     "user_tz": 300
    },
    "id": "KuMRvu9ceZvT",
    "outputId": "b05f63db-8001-42e7-fd82-0fada9b6a0a4"
   },
   "outputs": [],
   "source": [
    "# person id\n",
    "pid = (\"fashionWOMENBlouses_Shirtsid0000637003_1front.jpg\", None , None) # load person from the file\n",
    "\n",
    "# garments to try on (ordered)\n",
    "gids = [\n",
    "    (\"gfla\",2,5),\n",
    "    (\"strip\",3,1),\n",
    "       ]\n",
    "\n",
    "# garments to lay over (ordered)\n",
    "ogids = [\n",
    " (\"fashionWOMENTees_Tanksid0000159006_1front.jpg\", None ,5),\n",
    " ('fashionWOMENJackets_Coatsid0000645302_1front.jpg', None ,3),\n",
    "]\n",
    "\n",
    "# dressing in order\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid=pid, gids=gids, ogids=ogids)\n",
    "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnF5FnQBeXRI"
   },
   "source": [
    "## Opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 1493,
     "status": "ok",
     "timestamp": 1681790314174,
     "user": {
      "displayName": "Aiyu Cui",
      "userId": "13130403645654724387"
     },
     "user_tz": 300
    },
    "id": "G0yPTUqQaDA4",
    "outputId": "9962df5c-3060-4b2e-8e22-213ca4406ffc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dress_in_order_opcaity(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2]):\n",
    "    PID = [0,4,6,7]\n",
    "    # encode person\n",
    "    pimg, parse, from_pose = load_img(pid, ds)\n",
    "    if not pose_id:\n",
    "        to_pose = from_pose\n",
    "    else:\n",
    "        to_img, _, to_pose = load_img(pose_id, ds)\n",
    "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
    "\n",
    "    # encode base garments\n",
    "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
    "\n",
    "    # swap base garment if any\n",
    "    gimgs = []\n",
    "    for gid in gids:\n",
    "        gimg, gparse, pose =  load_img(gid, ds)\n",
    "        seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        gsegs[gid[2]] = seg\n",
    "        gimgs += [gimg]\n",
    "\n",
    "    # encode garment (overlay)\n",
    "    garments = []\n",
    "    over_gsegs = []\n",
    "    oimgs = []\n",
    "    for gid in ogids:\n",
    "        oimg, oparse, pose = load_img(gid, ds)\n",
    "        oimgs.append(oimg)\n",
    "        seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        over_gsegs += [seg]\n",
    "\n",
    "    gsegs = [gsegs[i] for i in order] + over_gsegs\n",
    "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "    gmap, mask = gsegs[-1]\n",
    "    gens = []\n",
    "    for alpha in [0, 0.99, 0.6, 0.35, 0.25]:\n",
    "        curr_mask = mask.clone() # * alpha\n",
    "        curr_mask[curr_mask >= alpha] = alpha\n",
    "        gsegs[-1] = gmap, curr_mask\n",
    "        img = model.netG(to_pose[None], psegs, gsegs, alpha=0.1999)\n",
    "        gens.append(img[0])\n",
    "    return pimg, gimgs, oimgs, gens, to_pose\n",
    "\n",
    "pid = ('fashionWOMENBlouses_Shirtsid0000270306_1front.jpg', None, 5)\n",
    "gids = [('fashionWOMENTees_Tanksid0000255303_1front.jpg', None, 5)]\n",
    "ogids = [('fashionWOMENBlouses_Shirtsid0000270306_1front.jpg', None, 5)]\n",
    "\n",
    "pimg, gimgs, oimgs, gens, to_pose = dress_in_order_opcaity(model, pid=pid, gids=gids, ogids=ogids)\n",
    "\n",
    "# plot results: (source person, source garment-inside, transparency results)\n",
    "output = torch.cat([pimg, gimgs[0],gens[0]] + gens[::-1][:-1], 2)\n",
    "output = (output + 1) / 2\n",
    "output = output.float().cpu().detach().numpy()\n",
    "output = np.transpose(output, [1,2,0])\n",
    "fig=plt.figure(figsize=(6,4), dpi= 200, facecolor='w', edgecolor='k')\n",
    "plt.imshow(output)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djcD8QndgZDg"
   },
   "source": [
    "## Reshaping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gnf7lQTfgb4Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "def dress_in_order_texshape(model, pid, target=5, shape_id=None, tex_patch=None, order=[2,5,1,3]):\n",
    "    PID = [0,4,6,7]\n",
    "    # encode person\n",
    "    pimg, parse, from_pose = load_img(pid, ds)\n",
    "    to_pose = from_pose\n",
    "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
    "\n",
    "    # encode base garments\n",
    "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
    "\n",
    "    fmap, mask = gsegs[target]\n",
    "    gimg = pimg*(parse==target)\n",
    "    if shape_id != None:\n",
    "        gimg, gparse, pose =  load_img(shape_id, ds)\n",
    "        _, mask = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=target)\n",
    "        shape_img = [gimg*(gparse==target)]\n",
    "    else:\n",
    "        shape_img = []\n",
    "    if tex_patch != None:\n",
    "        fmap = model.netE_attr(tex_patch, model.netVGG)\n",
    "    gsegs[target] = fmap, mask\n",
    "    gsegs = [gsegs[i] for i in order]\n",
    "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "    return pimg, [gimg], shape_img, gen_img[0], to_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 1218,
     "status": "ok",
     "timestamp": 1681790233951,
     "user": {
      "displayName": "Aiyu Cui",
      "userId": "13130403645654724387"
     },
     "user_tz": 300
    },
    "id": "TsRDArEwghnA",
    "outputId": "dcdf91d9-700a-4b14-8a6a-9c9d8c553600"
   },
   "outputs": [],
   "source": [
    "pid = ('print', 0, 5)\n",
    "shape_id = ('plain', 2, 5)\n",
    "\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order_texshape(model, pid, shape_id=shape_id)\n",
    "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI-it6f4g1kA"
   },
   "source": [
    "## Texture Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1681790290712,
     "user": {
      "displayName": "Aiyu Cui",
      "userId": "13130403645654724387"
     },
     "user_tz": 300
    },
    "id": "D5IwOnnZgsrm",
    "outputId": "51481b5b-d512-4b02-b493-a2c7053815eb"
   },
   "outputs": [],
   "source": [
    "\n",
    "pid = ('print', 0, 5)\n",
    "patch_id = ('plaid', 0, 5)\n",
    "patch, parse, from_pose = load_img(patch_id, ds)\n",
    "pimg, gimgs, oimgs, gen_img, pose = dress_in_order_texshape(model, pid, tex_patch=patch[None])\n",
    "plot_img(pimg, [patch], oimgs, gen_img, pose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zlQCwn6hGgT"
   },
   "source": [
    "## Print Insertion (RGBA image required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the function load_img and model are already defined elsewhere\n",
    "\n",
    "fn = \"data/doge.jpg\"\n",
    "pid = ('plain', 3, None)\n",
    "\n",
    "image = cv2.imread(fn, cv2.IMREAD_UNCHANGED)  # Read RGBA image\n",
    "if image.shape[2] == 3:  # If the image has only RGB channels\n",
    "    # Convert to RGBA by adding an alpha channel (fully opaque)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGRA)\n",
    "    \n",
    "# Put the print on a blank canvas\n",
    "x, y, h, w = 90, 60, 80, 70\n",
    "image = cv2.resize(image, (w, h))\n",
    "bg = np.zeros((256, 176, 4), dtype=np.uint8)  # Create an RGBA background\n",
    "bg[x:x + h, y:y + w] = image  # Paste the image on the background\n",
    "image = bg\n",
    "\n",
    "# Crop the print image and apply transparency mask\n",
    "trans_mask = image[:, :, 3] != 0  # Extract the alpha mask (non-zero alpha)\n",
    "image = image[:, :, 2::-1].transpose(2, 0, 1)  # Convert from BGRA to BGR (channels)\n",
    "image = (image / 255.0) * 2 - 1  # Normalize to [-1, 1]\n",
    "image = image * trans_mask[None]  # Apply the transparency mask\n",
    "\n",
    "# Run DiOr model (Assuming `load_img` and `model` are defined)\n",
    "pimg, parse, to_pose = load_img(pid, ds)\n",
    "psegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [0, 4, 6, 7])\n",
    "gsegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [5, 1, 2])\n",
    "\n",
    "# Insert the print into the image\n",
    "print_image = torch.from_numpy(image).float().to('mps')\n",
    "print_fmap = model.netE_attr(print_image[None], model.netVGG)\n",
    "print_mask = model.netE_attr.segmentor(print_fmap)\n",
    "gsegs = gsegs[:1] + [(print_fmap, torch.sigmoid(print_mask))] + gsegs[1:]\n",
    "\n",
    "# Generate the final image\n",
    "gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "# Construct a copy-and-paste image for comparison\n",
    "paste_img = image + pimg.cpu().detach().numpy() * (1 - trans_mask[None])\n",
    "paste_img = torch.from_numpy(paste_img).float().to('mps')\n",
    "\n",
    "# Display the output\n",
    "output = torch.cat([pimg, paste_img, gen_img[0]], 2)\n",
    "output = output.float().cpu().detach().numpy()\n",
    "output = (output + 1) / 2\n",
    "output = np.transpose((output * 255.0).astype(np.uint8), [1, 2, 0])\n",
    "\n",
    "# Save the result to a file instead of using plt.show()\n",
    "plt.figure(figsize=(6, 4), dpi=100, facecolor='w', edgecolor='k')\n",
    "plt.imshow(output)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"output_image.png\", dpi=100, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custome image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_parse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m person_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(person_image\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load or create parse and pose maps\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Assuming you have a function to generate parse maps and pose maps for your custom image\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Replace `generate_parse` and `generate_pose` with your actual preprocessing functions\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m parse_map \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_parse\u001b[49m(person_image)  \u001b[38;5;66;03m# Expected output: Tensor of shape (1, H, W)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m pose_map \u001b[38;5;241m=\u001b[39m generate_pose(person_image)    \u001b[38;5;66;03m# Expected output: Tensor of shape (1, C, H, W)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Replace load_img logic\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_parse' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your custom person image\n",
    "custom_image_path = \"data/images/MEN-Denim-id_00000080-01_7_additional.jpg\"  # Replace with your image path\n",
    "person_image = cv2.imread(custom_image_path)\n",
    "person_image = cv2.resize(person_image, (176, 256))  # Resize to match the expected size\n",
    "person_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "person_image = (person_image / 255.0) * 2 - 1  # Normalize to [-1, 1]\n",
    "person_image = torch.from_numpy(person_image.transpose(2, 0, 1)).float().unsqueeze(0).to('mps')\n",
    "\n",
    "# Load or create parse and pose maps\n",
    "# Assuming you have a function to generate parse maps and pose maps for your custom image\n",
    "# Replace `generate_parse` and `generate_pose` with your actual preprocessing functions\n",
    "parse_map = generate_parse(person_image)  # Expected output: Tensor of shape (1, H, W)\n",
    "pose_map = generate_pose(person_image)    # Expected output: Tensor of shape (1, C, H, W)\n",
    "\n",
    "# Replace load_img logic\n",
    "pimg = person_image.squeeze(0)  # Person image\n",
    "parse = parse_map.squeeze(0)    # Parse map\n",
    "to_pose = pose_map.squeeze(0)   # Pose map\n",
    "\n",
    "# Run DiOr model (Same as before)\n",
    "psegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [0, 4, 6, 7])\n",
    "gsegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [5, 1, 2])\n",
    "\n",
    "# Insert the print into the image\n",
    "print_image = torch.from_numpy(image).float().to('mps')\n",
    "print_fmap = model.netE_attr(print_image[None], model.netVGG)\n",
    "print_mask = model.netE_attr.segmentor(print_fmap)\n",
    "gsegs = gsegs[:1] + [(print_fmap, torch.sigmoid(print_mask))] + gsegs[1:]\n",
    "\n",
    "# Generate the final image\n",
    "gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "# Construct a copy-and-paste image for comparison\n",
    "paste_img = image + pimg.cpu().detach().numpy() * (1 - trans_mask[None])\n",
    "paste_img = torch.from_numpy(paste_img).float().to('mps')\n",
    "\n",
    "# Display the output\n",
    "output = torch.cat([pimg, paste_img, gen_img[0]], 2)\n",
    "output = output.float().cpu().detach().numpy()\n",
    "output = (output + 1) / 2\n",
    "output = np.transpose((output * 255.0).astype(np.uint8), [1, 2, 0])\n",
    "\n",
    "# Save the result\n",
    "plt.figure(figsize=(6, 4), dpi=100, facecolor='w', edgecolor='k')\n",
    "plt.imshow(output)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"output_image.png\", dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def generate_parse(image):\n",
    "    \"\"\"\n",
    "    Generate a segmentation map for the given person image.\n",
    "    Args:\n",
    "        image (torch.Tensor): Image of shape (1, 3, H, W) in [-1, 1] range.\n",
    "    Returns:\n",
    "        torch.Tensor: Parse map of shape (H, W) with class indices.\n",
    "    \"\"\"\n",
    "    # Transform the image back to [0, 1] and convert to PIL\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    img_pil = to_pil(((image[0] + 1) / 2).cpu())\n",
    "\n",
    "    # Pretrained model (e.g., DeepLabV3+ or similar)\n",
    "    model = torch.hub.load('pytorch/vision:v0.11.3', 'deeplabv3_resnet101', pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess the image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(img_pil).unsqueeze(0)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "    parse_map = output.argmax(0)  # Get the class index for each pixel\n",
    "    return parse_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/alokkumar/.cache/torch/hub/pytorch_vision_v0.11.3\n"
     ]
    }
   ],
   "source": [
    "custom_image_path = \"data/images/MEN-Denim-id_00000080-01_7_additional.jpg\"  # Replace with your image path\n",
    "person_image = cv2.imread(custom_image_path)\n",
    "\n",
    "# Preprocess the input image\n",
    "person_image = cv2.resize(person_image, (256, 256))  # Resize to expected size\n",
    "person_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "person_image = (person_image / 255.0) * 2 - 1  # Normalize to [-1, 1]\n",
    "person_image = torch.from_numpy(person_image.transpose(2, 0, 1)).float().unsqueeze(0)\n",
    "\n",
    "# Generate parse map\n",
    "parse_map = generate_parse(person_image)  # Assume this returns a map of shape (H, W)\n",
    "\n",
    "# Visualize and save the parse map\n",
    "parse_image = parse_map.cpu().numpy()  # Convert to NumPy\n",
    "parse_image = (parse_image / parse_image.max()) * 255  # Normalize to [0, 255]\n",
    "parse_image = parse_image.astype(np.uint8)\n",
    "\n",
    "# Save or display the image\n",
    "plt.imshow(parse_image, cmap='viridis')  # Use a colormap for better contrast\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Pose123.png\", dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1WfeKTPtt3qtlcTlrX47J03mxUzbVvyrL",
     "timestamp": 1733210810880
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09d7e1caadca461585867047d5990d36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4aaa6fd816354d00aaf5dfbb787521ff",
       "IPY_MODEL_148c216158054429965ac0fa9a522738",
       "IPY_MODEL_172df104dabc47ce97e4526eb8c56dd1"
      ],
      "layout": "IPY_MODEL_581eaef9466e4e0187a7a4dd86539e48"
     }
    },
    "148c216158054429965ac0fa9a522738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_645074065cba4dcca84ea4c51467d30b",
      "max": 40658,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c258e81d26334c71a7f2331ab9b04e94",
      "value": 40658
     }
    },
    "172df104dabc47ce97e4526eb8c56dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_687900a5fd6f4ba79c3a83a05c17fda2",
      "placeholder": "",
      "style": "IPY_MODEL_41663c7c73ba4764a37bb0c6e9e27552",
      "value": "40658/40658[00:06&lt;00:00,6345.53it/s]"
     }
    },
    "41663c7c73ba4764a37bb0c6e9e27552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4aaa6fd816354d00aaf5dfbb787521ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_781a90e9055f4e849694f54925153624",
      "placeholder": "",
      "style": "IPY_MODEL_510a903806dc49c5a1a1e5417b6f531e",
      "value": "100%"
     }
    },
    "510a903806dc49c5a1a1e5417b6f531e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "581eaef9466e4e0187a7a4dd86539e48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645074065cba4dcca84ea4c51467d30b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "687900a5fd6f4ba79c3a83a05c17fda2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "781a90e9055f4e849694f54925153624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c258e81d26334c71a7f2331ab9b04e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
